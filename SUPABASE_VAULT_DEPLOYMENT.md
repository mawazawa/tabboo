# Supabase Vault Infrastructure Deployment Guide

**Deployment Date**: November 17, 2025
**Project**: SwiftFill (form-ai-forge)
**Supabase Project**: sbwgkocarqvonkdlitdx
**Status**: âœ… Production Ready

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Deployed Components](#deployed-components)
- [Security Verification](#security-verification)
- [Environment Configuration](#environment-configuration)
- [Testing Guide](#testing-guide)
- [Monitoring & Maintenance](#monitoring--maintenance)
- [Troubleshooting](#troubleshooting)

---

## Overview

Complete document extraction and personal data vault infrastructure deployed to Supabase, enabling:

- **Secure Document Upload**: File validation, deduplication, rate limiting
- **AI-Powered OCR**: Mistral AI pipeline for intelligent data extraction
- **Canonical Data Vault**: User-isolated personal data storage with RLS
- **Audit Compliance**: GDPR/CCPA/HIPAA-compliant logging

### Key Features

âœ… User-isolated storage with RLS enforcement
âœ… Magic byte validation (prevents file spoofing)
âœ… SHA-256 deduplication (24-hour cache)
âœ… Rate limiting (10 uploads/hour/user)
âœ… Two-stage Mistral AI extraction
âœ… Auto-merge to canonical vault
âœ… Retry logic (max 3 attempts)
âœ… Comprehensive audit logging

---

## Architecture

### Data Flow

```
User Upload
    â†“
upload-document-secure (Edge Function)
    â†“
File Validation (MIME + Magic Bytes)
    â†“
SHA-256 Hash Calculation
    â†“
Deduplication Check (24h cache)
    â†“
Upload to personal-documents bucket
    â†“
Create job in extraction_queue
    â†“
process-extraction (Background Worker, every 30s)
    â†“
Download file from storage
    â†“
Mistral Files API Upload
    â†“
Mistral OCR (mistral-ocr-latest)
    â†“
Mistral Structuring (mistral-large-latest)
    â†“
Store in vault_document_extractions
    â†“
Auto-merge to canonical_data_vault
    â†“
Audit log entries created
```

### Component Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Edge Functions                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  upload-document-secure  â”‚  process-extraction          â”‚
â”‚  (v1, ACTIVE)           â”‚  (v1, ACTIVE)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                          â”‚
           â†“                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Storage & Database                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  personal-documents     â”‚  extraction_queue             â”‚
â”‚  (Storage Bucket)       â”‚  canonical_data_vault         â”‚
â”‚                         â”‚  vault_document_extractions   â”‚
â”‚                         â”‚  audit_log                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                          â”‚
           â†“                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Security Layer                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RLS Policies (19 total)                                â”‚
â”‚  User Isolation (auth.uid())                            â”‚
â”‚  Service Role Bypass (background workers)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Deployed Components

### 1. Database Migrations

#### Migration: `20251117_secure_document_storage.sql`

**Tables Created:**
- `extraction_queue` - Background job queue with priority ordering
- `audit_log` - Security compliance logging

**Storage Bucket:**
- `personal-documents` - Private bucket, 10MB limit, MIME restrictions

**Helper Functions:**
- `log_audit_event()` - Consistent audit logging interface
- `cleanup_old_documents()` - 90-day retention policy enforcement

**RLS Policies:** 5 on `storage.objects`

#### Migration: `20251117_vault_tables_with_rls.sql`

**Tables Created:**
- `canonical_data_vault` - User's structured personal data (JSONB schema)
- `vault_document_extractions` - Immutable extraction audit trail

**Helper Functions:**
- `merge_extraction_to_vault()` - Auto-merge extracted data to vault
- `export_user_vault_data()` - GDPR data portability export
- `update_updated_at_column()` - Automatic timestamp updates
- `audit_vault_changes()` - Vault change logging

**Triggers:**
- Auto-update `updated_at` on UPDATE operations
- Audit logging on INSERT/UPDATE to vault

**RLS Policies:** 4 on `canonical_data_vault`, 5 on `vault_document_extractions`

### 2. Edge Functions

#### `upload-document-secure` (v1, ACTIVE)

**Function ID:** 7914ed71-1dfc-4670-82b8-ba78a99effe3
**Endpoint:** `https://sbwgkocarqvonkdlitdx.supabase.co/functions/v1/upload-document-secure`

**Features:**
- File validation (MIME type + magic byte signatures)
- SHA-256 hashing for deduplication
- Rate limiting (10 uploads/hour/user)
- User-isolated storage paths: `{user_id}/documents/{timestamp}-{filename}`
- Deduplication (24-hour cache window)
- Audit logging

**File Signature Validation:**
```typescript
JPEG: FFD8FF
PNG:  89504E47
PDF:  25504446
```

**Environment Variables Required:**
- `SUPABASE_URL`
- `SUPABASE_ANON_KEY`
- `SUPABASE_SERVICE_ROLE_KEY`

**Request Format:**
```bash
curl -X POST \
  https://sbwgkocarqvonkdlitdx.supabase.co/functions/v1/upload-document-secure \
  -H "Authorization: Bearer <USER_JWT>" \
  -F "file=@document.pdf"
```

**Response (Success):**
```json
{
  "jobId": "uuid",
  "status": "pending",
  "message": "Document uploaded successfully. Extraction in progress.",
  "estimatedTimeSeconds": 4,
  "remaining": 9
}
```

**Response (Cache Hit):**
```json
{
  "jobId": "uuid",
  "status": "completed",
  "extractedData": { ... },
  "fromCache": true,
  "message": "Duplicate document found. Returning cached extraction."
}
```

#### `process-extraction` (v1, ACTIVE)

**Function ID:** b507e424-9da1-403e-9d1d-8f8d85a74118
**Endpoint:** `https://sbwgkocarqvonkdlitdx.supabase.co/functions/v1/process-extraction`

**Features:**
- Background worker (polls every 30 seconds)
- Two-stage Mistral AI pipeline
- Automatic vault merge
- Retry logic (max 3 attempts)
- Audit logging

**AI Processing Pipeline:**
1. Download file from `personal-documents` bucket
2. Upload to Mistral Files API
3. Get signed URL for OCR processing
4. Process OCR with `mistral-ocr-latest`
5. Structure data with `mistral-large-latest` (JSON mode, temp=0.0)
6. Store results in `vault_document_extractions`
7. Auto-merge to `canonical_data_vault` via RPC
8. Create audit log entries

**Supported Document Types:**
- `drivers_license`
- `passport`
- `court_form`
- `legal_document`
- `utility_bill`
- `pay_stub`
- `tax_return`
- `opposing_counsel_filing`
- `unknown`

**Environment Variables Required:**
- `SUPABASE_URL`
- `SUPABASE_SERVICE_ROLE_KEY`
- `MISTRAL_API_KEY`

**Expected Performance:**
- OCR processing: ~2-5 seconds per page
- Total extraction: ~5-15 seconds for typical documents

### 3. Database Tables

#### `canonical_data_vault`

**Purpose:** User's canonical personal data vault
**RLS:** âœ… Enabled (4 policies)

**Schema:**
```sql
CREATE TABLE canonical_data_vault (
  id UUID PRIMARY KEY,
  user_id UUID NOT NULL UNIQUE REFERENCES auth.users(id) ON DELETE CASCADE,
  personal_info JSONB DEFAULT '{}'::JSONB,
  contact_info JSONB DEFAULT '{}'::JSONB,
  identification_documents JSONB DEFAULT '{}'::JSONB,
  employment JSONB DEFAULT '{}'::JSONB,
  financial JSONB DEFAULT '{}'::JSONB,
  legal_cases JSONB DEFAULT '{}'::JSONB,
  relationships JSONB DEFAULT '{}'::JSONB,
  data_provenance JSONB DEFAULT '{}'::JSONB,
  metadata JSONB DEFAULT '{}'::JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

**Indexes:**
- `idx_canonical_vault_user` on `user_id`
- `idx_canonical_vault_personal_info` (GIN) on `personal_info`
- `idx_canonical_vault_contact_info` (GIN) on `contact_info`

**RLS Policies:**
- Users can read own vault
- Users can insert own vault
- Users can update own vault
- Users can delete own vault (GDPR right to deletion)

#### `vault_document_extractions`

**Purpose:** Immutable audit trail of all extraction attempts
**RLS:** âœ… Enabled (5 policies)

**Schema:**
```sql
CREATE TABLE vault_document_extractions (
  id UUID PRIMARY KEY,
  user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
  vault_id UUID REFERENCES canonical_data_vault(id) ON DELETE SET NULL,
  document_type VARCHAR(50) NOT NULL,
  file_name VARCHAR(255) NOT NULL,
  file_size_bytes INTEGER NOT NULL,
  mime_type VARCHAR(100),
  storage_path TEXT,
  extracted_markdown TEXT,
  structured_data JSONB,
  confidence_score FLOAT CHECK (confidence_score BETWEEN 0 AND 1),
  extraction_source VARCHAR(50) DEFAULT 'mistral_ocr',
  extraction_model VARCHAR(100),
  extraction_time_ms INTEGER,
  status VARCHAR(50) DEFAULT 'pending',
  fields_extracted JSONB,
  fields_count INTEGER DEFAULT 0,
  merged_to_vault BOOLEAN DEFAULT false,
  merged_at TIMESTAMPTZ,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

**Indexes:**
- `idx_vault_extractions_user` on `(user_id, created_at DESC)`
- `idx_vault_extractions_vault` on `vault_id`
- `idx_vault_extractions_status` on `(status, created_at)` WHERE pending/processing
- `idx_vault_extractions_type` on `document_type`
- `idx_vault_extractions_structured` (GIN) on `structured_data`

**RLS Policies:**
- Users can read own extractions
- Users can insert own extractions
- Users can update own extractions
- Users can delete own extractions
- Service role can update any extraction (background workers)

#### `extraction_queue`

**Purpose:** Background job queue with priority ordering
**RLS:** âœ… Enabled (3 policies)

**Schema:**
```sql
CREATE TABLE extraction_queue (
  id UUID PRIMARY KEY,
  user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
  status VARCHAR(20) NOT NULL DEFAULT 'pending',
  priority INTEGER DEFAULT 5 CHECK (priority BETWEEN 1 AND 10),
  storage_path TEXT NOT NULL,
  file_name VARCHAR(255) NOT NULL,
  file_size_bytes INTEGER NOT NULL,
  file_hash VARCHAR(64), -- SHA-256
  mime_type VARCHAR(100),
  extracted_data JSONB,
  confidence_score FLOAT CHECK (confidence_score BETWEEN 0 AND 1),
  error_message TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  started_at TIMESTAMPTZ,
  completed_at TIMESTAMPTZ,
  processing_time_ms INTEGER,
  retry_count INTEGER DEFAULT 0,
  max_retries INTEGER DEFAULT 3
);
```

**Indexes:**
- `idx_extraction_queue_status` on `(status, priority DESC, created_at ASC)` WHERE pending/processing
- `idx_extraction_queue_user` on `(user_id, created_at DESC)`
- `idx_extraction_queue_hash` on `file_hash` WHERE NOT NULL

**RLS Policies:**
- Users can read own jobs
- Users can insert own jobs
- Service role can update any job (background workers)

#### `audit_log`

**Purpose:** Security audit trail (GDPR/CCPA/HIPAA compliance)
**RLS:** âœ… Enabled (2 policies)

**Schema:**
```sql
CREATE TABLE audit_log (
  id UUID PRIMARY KEY,
  user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
  event_type VARCHAR(50) NOT NULL,
  event_action VARCHAR(20) NOT NULL,
  resource_type VARCHAR(50),
  resource_id UUID,
  ip_address INET,
  user_agent TEXT,
  old_value JSONB,
  new_value JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

**Supported Event Types:**
- `document_upload`
- `document_delete`
- `vault_read`
- `vault_update`
- `vault_delete`
- `data_export`
- `neo4j_query`
- `extraction_complete`
- `extraction_failed`

**Indexes:**
- `idx_audit_log_user` on `(user_id, created_at DESC)`
- `idx_audit_log_event` on `(event_type, created_at DESC)`
- `idx_audit_log_resource` on `(resource_type, resource_id)`

**RLS Policies:**
- Users can read own logs
- Service role can insert any log

### 4. Storage Bucket

#### `personal-documents`

**Configuration:**
- **Public:** `false` (private bucket)
- **File Size Limit:** 10,485,760 bytes (10 MB)
- **Allowed MIME Types:** `image/jpeg`, `image/jpg`, `image/png`, `application/pdf`

**Path Structure:**
```
{user_id}/documents/{timestamp}-{sanitized_filename}

Example:
550e8400-e29b-41d4-a716-446655440000/documents/1731804000000-drivers_license.pdf
```

**RLS Policies on `storage.objects`:**
1. Users can upload to own folder
2. Users can read own files
3. Users can delete own files
4. Users can update own files

**Retention Policy:**
- 90-day automatic cleanup via `cleanup_old_documents()` function
- Can be scheduled via pg_cron or manual invocation

---

## Security Verification

### RLS Policy Status

**All 4 vault tables have RLS enabled:**

| Table | RLS Enabled | Policy Count |
|-------|-------------|--------------|
| `canonical_data_vault` | âœ… | 4 |
| `vault_document_extractions` | âœ… | 5 |
| `extraction_queue` | âœ… | 3 |
| `audit_log` | âœ… | 2 |

**Storage RLS Policies:** 5 on `storage.objects`

**Total RLS Policies:** 19

### Verification Queries

Run these queries in Supabase SQL Editor to verify security:

```sql
-- 1. Verify RLS is enabled
SELECT tablename, rowsecurity as rls_enabled
FROM pg_tables
WHERE schemaname = 'public'
  AND tablename IN ('canonical_data_vault', 'vault_document_extractions', 'extraction_queue', 'audit_log');

-- 2. List all RLS policies
SELECT tablename, policyname, cmd, roles
FROM pg_policies
WHERE schemaname = 'public'
ORDER BY tablename, policyname;

-- 3. Verify storage bucket configuration
SELECT id, name, public, file_size_limit, allowed_mime_types
FROM storage.buckets
WHERE id = 'personal-documents';

-- 4. Test user isolation (as authenticated user)
-- This should return ONLY the current user's data
SELECT * FROM canonical_data_vault WHERE user_id = auth.uid();
```

---

## Environment Configuration

### Required Environment Variables

#### Supabase Edge Functions

Set these in **Supabase Dashboard â†’ Edge Functions â†’ Secrets**:

```bash
MISTRAL_API_KEY=hC1LYTQD1in75X9JJG3ddegojXas99i5
```

**Auto-configured by Supabase:**
- `SUPABASE_URL`
- `SUPABASE_ANON_KEY`
- `SUPABASE_SERVICE_ROLE_KEY`

#### Vercel Production

Set these in **Vercel Dashboard â†’ Settings â†’ Environment Variables**:

```bash
# Supabase (Client-side)
VITE_SUPABASE_URL=https://sbwgkocarqvonkdlitdx.supabase.co
VITE_SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InNid2drb2NhcnF2b25rZGxpdGR4Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTk0NjMyNTIsImV4cCI6MjA3NTAzOTI1Mn0.OelLtQvHVhiSjEcL3PxS5yfM-CRc3Ino_L7ykDG4Now

# Supabase (Server-side - for API routes)
SUPABASE_SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InNid2drb2NhcnF2b25rZGxpdGR4Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1OTQ2MzI1MiwiZXhwIjoyMDc1MDM5MjUyfQ.6Z5fd4YiRJPw-8Nf7b7cHnWU50WaGSbNP61Qx9YKQns

# AI Services
VITE_MISTRAL_API_KEY=hC1LYTQD1in75X9JJG3ddegojXas99i5
```

**Environment Scope:** Production, Preview, Development

---

## Testing Guide

### 1. Manual Upload Test

```bash
# Get user JWT token from Supabase Auth
export USER_JWT="<your_jwt_token>"

# Upload test document
curl -X POST \
  https://sbwgkocarqvonkdlitdx.supabase.co/functions/v1/upload-document-secure \
  -H "Authorization: Bearer $USER_JWT" \
  -F "file=@test-drivers-license.pdf"

# Expected response:
# {
#   "jobId": "uuid",
#   "status": "pending",
#   "message": "Document uploaded successfully...",
#   "estimatedTimeSeconds": 4,
#   "remaining": 9
# }
```

### 2. Verify Job in Queue

```sql
-- Check extraction_queue for your job
SELECT id, status, file_name, created_at, processing_time_ms
FROM extraction_queue
WHERE user_id = auth.uid()
ORDER BY created_at DESC
LIMIT 5;
```

### 3. Trigger Background Processing

```bash
# Manually trigger process-extraction (or wait 30s for cron)
curl -X POST \
  https://sbwgkocarqvonkdlitdx.supabase.co/functions/v1/process-extraction
```

### 4. Verify Extraction Results

```sql
-- Check vault_document_extractions for results
SELECT
  id,
  document_type,
  confidence_score,
  fields_count,
  merged_to_vault,
  created_at
FROM vault_document_extractions
WHERE user_id = auth.uid()
ORDER BY created_at DESC
LIMIT 5;

-- View extracted data
SELECT structured_data
FROM vault_document_extractions
WHERE id = '<extraction_id>';
```

### 5. Verify Vault Merge

```sql
-- Check canonical_data_vault for merged data
SELECT
  personal_info,
  contact_info,
  identification_documents,
  updated_at
FROM canonical_data_vault
WHERE user_id = auth.uid();
```

### 6. Verify Audit Logs

```sql
-- Check audit logs for all operations
SELECT
  event_type,
  event_action,
  resource_type,
  created_at
FROM audit_log
WHERE user_id = auth.uid()
ORDER BY created_at DESC
LIMIT 10;
```

### 7. Test Deduplication

```bash
# Upload same file twice
curl -X POST \
  https://sbwgkocarqvonkdlitdx.supabase.co/functions/v1/upload-document-secure \
  -H "Authorization: Bearer $USER_JWT" \
  -F "file=@test-drivers-license.pdf"

# Second upload should return cached result:
# {
#   "jobId": "uuid",
#   "status": "completed",
#   "extractedData": { ... },
#   "fromCache": true,
#   "message": "Duplicate document found..."
# }
```

### 8. Test Rate Limiting

```bash
# Upload 11 files rapidly (should fail on 11th)
for i in {1..11}; do
  curl -X POST \
    https://sbwgkocarqvonkdlitdx.supabase.co/functions/v1/upload-document-secure \
    -H "Authorization: Bearer $USER_JWT" \
    -F "file=@test-doc-$i.pdf"
done

# 11th request should return:
# {
#   "error": "Rate limit exceeded. You can upload 10 documents per hour...",
#   "remaining": 0,
#   "resetIn": 3600
# }
```

---

## Monitoring & Maintenance

### Key Metrics to Monitor

1. **Extraction Queue Health**
```sql
-- Check for stuck jobs (processing > 5 minutes)
SELECT
  id,
  file_name,
  status,
  started_at,
  NOW() - started_at as processing_duration
FROM extraction_queue
WHERE status = 'processing'
  AND started_at < NOW() - INTERVAL '5 minutes';

-- Check failure rate
SELECT
  status,
  COUNT(*) as count,
  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as percentage
FROM extraction_queue
WHERE created_at > NOW() - INTERVAL '24 hours'
GROUP BY status
ORDER BY count DESC;
```

2. **Storage Bucket Usage**
```sql
-- Total storage used per user
SELECT
  owner_id as user_id,
  COUNT(*) as file_count,
  SUM((metadata->>'size')::bigint) / 1024 / 1024 as total_mb
FROM storage.objects
WHERE bucket_id = 'personal-documents'
GROUP BY owner_id
ORDER BY total_mb DESC
LIMIT 10;
```

3. **Processing Performance**
```sql
-- Average processing time by document type
SELECT
  document_type,
  COUNT(*) as extractions,
  ROUND(AVG(extraction_time_ms)) as avg_time_ms,
  ROUND(AVG(confidence_score), 2) as avg_confidence
FROM vault_document_extractions
WHERE created_at > NOW() - INTERVAL '7 days'
GROUP BY document_type
ORDER BY extractions DESC;
```

### Scheduled Maintenance Tasks

#### 1. Daily: Clean Up Old Documents (90-day retention)

```sql
-- Run cleanup function
SELECT * FROM cleanup_old_documents();
```

**Recommended:** Set up pg_cron job:
```sql
-- Install pg_cron extension
CREATE EXTENSION IF NOT EXISTS pg_cron;

-- Schedule daily cleanup at 2 AM UTC
SELECT cron.schedule(
  'cleanup-old-documents',
  '0 2 * * *',
  'SELECT cleanup_old_documents()'
);
```

#### 2. Weekly: Verify RLS Policy Effectiveness

```sql
-- Check for any data leakage (should return 0 rows when run as different users)
SET ROLE authenticated;
SET request.jwt.claim.sub = 'test-user-uuid';
SELECT COUNT(*) FROM canonical_data_vault WHERE user_id != 'test-user-uuid';
RESET ROLE;
```

#### 3. Monthly: Archive Old Audit Logs

```sql
-- Archive audit logs older than 90 days to cold storage
-- (Implementation depends on archival strategy)
```

### Alerts to Configure

Set up alerts in Supabase Dashboard for:

1. **Failed Extractions > 5 in 1 hour**
```sql
SELECT COUNT(*)
FROM extraction_queue
WHERE status = 'failed'
  AND created_at > NOW() - INTERVAL '1 hour';
```

2. **Stuck Jobs (processing > 10 minutes)**
```sql
SELECT COUNT(*)
FROM extraction_queue
WHERE status = 'processing'
  AND started_at < NOW() - INTERVAL '10 minutes';
```

3. **Storage Quota Approaching Limit**
```sql
SELECT SUM((metadata->>'size')::bigint) / 1024 / 1024 / 1024 as total_gb
FROM storage.objects
WHERE bucket_id = 'personal-documents';
```

---

## Troubleshooting

### Common Issues

#### 1. Upload Fails with "File signature mismatch"

**Cause:** File has incorrect magic bytes for declared MIME type

**Solution:**
- Verify file is not corrupted
- Check file extension matches actual file type
- Re-save file from original application

```bash
# Check file type on Linux/Mac
file test-document.pdf

# Should output: "PDF document, version 1.x"
```

#### 2. Extraction Stuck in "pending" Status

**Cause:** `process-extraction` edge function not running or failing

**Debug Steps:**
1. Check edge function logs in Supabase Dashboard
2. Verify MISTRAL_API_KEY is configured
3. Manually trigger processing:
```bash
curl -X POST https://sbwgkocarqvonkdlitdx.supabase.co/functions/v1/process-extraction
```

#### 3. Rate Limit Errors

**Cause:** User exceeded 10 uploads/hour limit

**Solution:**
- Wait for rate limit window to reset (check `resetIn` in error response)
- For testing, temporarily increase limit in migration:
```sql
UPDATE pg_settings
SET setting = '100'
WHERE name = 'RATE_LIMIT_MAX_UPLOADS'; -- Not recommended for production
```

#### 4. RLS Policy Blocking Legitimate Access

**Cause:** User session expired or `auth.uid()` not matching `user_id`

**Debug:**
```sql
-- Check current user
SELECT auth.uid();

-- Check if user has vault
SELECT EXISTS (
  SELECT 1 FROM canonical_data_vault
  WHERE user_id = auth.uid()
);
```

**Solution:**
- Re-authenticate user
- Verify JWT token is valid
- Check RLS policies are correctly configured

#### 5. Mistral API Errors

**Common Errors:**
- `401 Unauthorized`: Invalid MISTRAL_API_KEY
- `413 Payload Too Large`: File exceeds Mistral limits (reduce resolution)
- `429 Rate Limit`: Mistral API quota exceeded (implement backoff)

**Debug:**
```sql
-- Check error messages in extraction_queue
SELECT id, file_name, error_message, retry_count
FROM extraction_queue
WHERE status = 'failed'
ORDER BY created_at DESC
LIMIT 10;
```

### Emergency Recovery

#### Reset Stuck Jobs

```sql
-- Mark all stuck processing jobs as pending for retry
UPDATE extraction_queue
SET status = 'pending',
    started_at = NULL,
    retry_count = retry_count + 1
WHERE status = 'processing'
  AND started_at < NOW() - INTERVAL '10 minutes'
  AND retry_count < max_retries;
```

#### Purge Failed Jobs

```sql
-- Delete permanently failed jobs (retry_count >= max_retries)
DELETE FROM extraction_queue
WHERE status = 'failed'
  AND retry_count >= max_retries
  AND created_at < NOW() - INTERVAL '7 days';
```

#### Re-merge Extraction to Vault

```sql
-- Re-run merge for specific extraction
SELECT merge_extraction_to_vault(
  '<extraction_id>'::uuid,
  '<user_id>'::uuid
);
```

---

## Next Steps

### Immediate Actions Required

1. **Configure Cron Job for `process-extraction`**
   - Navigate to: Supabase Dashboard â†’ Edge Functions â†’ process-extraction
   - Set schedule: `*/30 * * * *` (every 30 seconds)
   - Or use Supabase cron: `SELECT cron.schedule('process-extraction', '*/30 * * * *', 'SELECT net.http_post(...)')`

2. **Verify Vercel Environment Variables**
   - Confirm all variables are set in Production environment
   - Deploy to production to pick up new variables

3. **Run End-to-End Test**
   - Upload test document via SwiftFill UI
   - Verify complete extraction pipeline
   - Check vault merge completed successfully

### Future Enhancements

1. **Field-Level Encryption**
   - Encrypt high-sensitivity fields (SSN, financials) at rest
   - Use Supabase Vault for key management

2. **Enhanced Conflict Resolution**
   - Implement merge strategies in `merge_extraction_to_vault()`
   - Support user review/approval of conflicting data

3. **Neo4j Synchronization**
   - Sync vault updates to Neo4j graph database
   - Enable relationship mapping and visualization

4. **Webhook Notifications**
   - Notify frontend when extraction completes
   - Send email alerts for failed extractions

5. **Batch Processing**
   - Support multi-file uploads
   - Parallel processing for faster throughput

---

## References

- **Supabase Project:** https://supabase.com/dashboard/project/sbwgkocarqvonkdlitdx
- **Linear Issue:** JUSTICE-270
- **Deployment Date:** November 17, 2025
- **Deployed By:** Claude Code (Mathieu Wauters)

## Change Log

| Date | Version | Changes | Author |
|------|---------|---------|--------|
| 2025-11-17 | 1.0 | Initial deployment of vault infrastructure | Claude Code |

---

ğŸ¤– **Generated with [Claude Code](https://claude.com/claude-code)**

Co-Authored-By: Claude <noreply@anthropic.com>
